# -*- coding: utf-8 -*-
"""Preprocess - Evaluasi MLOps.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dUPMhRUtMXgs0Ny6IOeewpOMAgc6L2_D

# Muat data
"""

import pandas as pd
data = pd.read_csv('heart_disease_cleaned.csv')
data.head()

"""# Drop Kolom Tidak Perlu"""

data = data.drop(columns=['id','dataset'])

data.head()

"""No"""

from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder, MinMaxScaler
from sklearn.compose import ColumnTransformer

# Separate features and target
X = data.drop(columns=['num'])
y = data['num']

# Define column categories
numerical_cols = ['age', 'trestbps', 'chol', 'thalch', 'oldpeak']
binary_cols = ['sex', 'fbs', 'exang']
ordinal_cols = ['cp', 'slope']
onehot_cols = ['restecg', 'thal', 'ca']

# Column transformer for initial encoding
preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), numerical_cols),
        ('bin_ord', OrdinalEncoder(), binary_cols + ordinal_cols),
        ('onehot', OneHotEncoder(drop='first'), onehot_cols)
    ])

# Fit and transform features
X_encoded = preprocessor.fit_transform(X)
print("Hasil setelah encoding awal: ")
X_encoded

# Scale all features uniformly using MinMaxScaler
scaler = MinMaxScaler()
X_scaled = scaler.fit_transform(X_encoded)

# Get feature names
ohe = preprocessor.named_transformers_['onehot']
onehot_feature_names = ohe.get_feature_names_out(onehot_cols)
all_columns = numerical_cols + binary_cols + ordinal_cols + list(onehot_feature_names)

# Create DataFrame
X_final = pd.DataFrame(X_scaled, columns=all_columns)
X_final['num'] = y.values

# Save to CSV
#final_output_path = "/mnt/data/processed_scaled_heart_disease.csv"
#X_final.to_csv(final_output_path, index=False)

# Show first few rows
X_final.describe()

X_final.head()

"""PELATIHAN MODEL"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
import seaborn as sns
import matplotlib.pyplot as plt
from xgboost import XGBClassifier

# 1. Load preprocessed dataset
df = X_final

# 2. Pisahkan fitur dan target
X = df.drop(columns=["num"])
y = df["num"]

# 3. Bagi data menjadi training dan testing (80:20)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, stratify=y, random_state=42
)

from imblearn.over_sampling import SMOTE
from xgboost import XGBClassifier
from sklearn.metrics import confusion_matrix, classification_report
from collections import Counter

# Terapkan SMOTE ke data training
smote = SMOTE(random_state=42)
X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)

# Cek distribusi baru
print("Distribusi label setelah SMOTE:", Counter(y_train_smote))

from sklearn.model_selection import GridSearchCV
from xgboost import XGBClassifier
from sklearn.metrics import confusion_matrix, classification_report

# Definisikan parameter grid
param_grid = {
    'max_depth': [3, 5, 7],
    'n_estimators': [100, 200],
    'learning_rate': [0.01, 0.1, 0.2],
    'subsample': [0.8, 1.0]
}

# Model dasar
xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42)

# Grid Search
grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid,
                           scoring='f1_weighted', cv=3, verbose=1, n_jobs=-1)

# Latih dengan data hasil SMOTE
grid_search.fit(X_train_smote, y_train_smote)

# Evaluasi model terbaik
best_model = grid_search.best_estimator_
y_pred_best = best_model.predict(X_test)

# Cetak hasil
print("Best Parameters:", grid_search.best_params_)
print("\nConfusion Matrix:")
print(confusion_matrix(y_test, y_pred_best))
print("\nClassification Report:")
print(classification_report(y_test, y_pred_best, zero_division=0))

# 6. Confusion Matrix
cm = confusion_matrix(y_test, y_pred_best)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.title("Confusion Matrix - XGBoost")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.tight_layout()
plt.show()

# 1. Distribusi label
label_distribution = y.value_counts().sort_index()

plt.figure(figsize=(8, 4))
sns.barplot(x=label_distribution.index, y=label_distribution.values, palette="viridis")
plt.title("Distribusi Label Target (num)")
plt.xlabel("Tingkat Keparahan Penyakit Jantung")
plt.ylabel("Jumlah Data")
plt.tight_layout()
plt.show()

print(label_distribution)

import joblib

# Simpan model ke file
joblib.dump(best_model, 'model.pkl')

# Jika ingin menyimpan preprocessor juga:
joblib.dump(preprocessor, 'preprocessor.pkl')
joblib.dump(scaler, 'scaler.pkl')